{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import kenlm\n",
    "global orig_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Probability Based Tokenizer\n",
    "\n",
    "Greedy tokenizer would generally work for most of the cases, however, it could lead to an undesirable segmentation, due to the preference towards longer chunks.\n",
    "We propose a joint consideration for sub-word segmentation by considering both source and target sentences.\n",
    "\n",
    "A translator needs a source sentence $\\mathbf{S}$ consisting of segmentations where $\\mathbf{S} = s_0 s_1 \\dots s_n$ and a target sentence $\\mathbf{T}$ consisting of segmentations where $\\mathbf{T} = t_0 t_1 \\dots t_m$.\n",
    "\n",
    "We want to find optimal arrangement of $\\mathbf{S}$ which is $\\mathbf{S}^*$ and optimal arrangement of $\\mathbf{T}$ which is $\\mathbf{T}^*$. Mathematically:\n",
    "\\begin{align}\n",
    "\\label{eq1}\n",
    "    \\mathbf{S}^*, \\mathbf{T}^* = \\underset{{s_i \\in \\mathbf{S}, t_j \\in \\mathbf{T}}}{\\operatorname{argmax}} P(\\mathbf{S}, \\mathbf{T})\n",
    "\\end{align}\n",
    "where $P(\\mathbf{S}, \\mathbf{T})$ is the joint probability of sequences. \n",
    "\n",
    "We assume that the prior probabilities, which are $P(\\mathbf{S})$ and $P(\\mathbf{T})$, are language model based probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_tokenizer(sentence):\n",
    "    s = sentence\n",
    "    global orig_len\n",
    "    orig_len = len(s)\n",
    "    return segment(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kenlm.Model(\"sim_train.klm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Memoize function f, whose args must all be hashable.\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(text, start=0, L=20):\n",
    "    \"Return a list of all (first, rest) pairs; start <= len(first) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose Viterbi for segmentation the given sentence. The scoring function is obtained from the constructed language models.\n",
    "\n",
    "Optimal segmentation depends on the following:\n",
    "<ol>\n",
    "    <li>language model score of source sentence of a candidate segment.</li>\n",
    "    <li>language model score of target sentence of a candidate segment.</li>\n",
    "    <li>item mapping conversions from source segment to target segment</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the most probable segmentation of text.\"\n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=Pwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid OOVs as output segmentations, we imposed a penalty on OOV outputs, which is given by: $\\alpha \\times \\frac{\\texttt{len(segment)}}{\\texttt{len(sentence)}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_constant = 15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords(words):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    sentence = \" \".join(words)\n",
    "    score = 0\n",
    "    words_ = ['<s>'] + sentence.split() + ['</s>']\n",
    "    for i, (prob, length, oov) in enumerate(model.full_scores(sentence)):\n",
    "        if oov:\n",
    "            penalty = len(words_[i+1]) / orig_len\n",
    "            score += penalty_constant * prob * penalty\n",
    "        else:\n",
    "            score += prob\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "alphanumerics = 'a-zA-Z0-9'\n",
    "known_stops = u'。。…！？'\n",
    "known_punctuation = u'／（）、，。：「」…。『』！？《》“”；’ ‘【】·〔〕'\n",
    "eng_punct = string.punctuation\n",
    "avoid = re.compile(\"([%s%s%s%s]+)\" % (alphanumerics, known_stops, known_punctuation, eng_punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Sentence\n",
    "Tokenize sentence and output the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    split_words = re.split(avoid, sentence)\n",
    "    split_words_values = [(i, bool(re.search(avoid, i))) for i in split_words]\n",
    "    answer = []\n",
    "    for (word, value) in split_words_values:\n",
    "        segmented_text = []\n",
    "        if value == False:\n",
    "            orig_len = len(word)\n",
    "            segmented_text = dp_tokenizer(word)\n",
    "        else:\n",
    "            segmented_text = list(word)\n",
    "        for segs in segmented_text:\n",
    "            answer.append(segs)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sentence and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"姚松炎、周庭势被「DQ」? 泛民质疑，政府再取消参选人资格涉政治筛选，要求律政司司长郑若骅解释法律理据。 有报道指，据全国人大常委会就《基本法》第一百零四条进行的释法，代表泛民参选立法会港岛及九龙西补选的香港众志周庭和被「DQ」前议员姚松炎，势被取消参选资格。律政司表示，法律政策专员黄惠冲将于稍后时间与泛民议员会面，确实时间待定。 民主派议员前晚在律政中心外静坐要求与律政司司长郑若骅会面不果后，昨在立法会召开记者招待会，要求郑就撤销参选人资格的理据，及其给予选举主任的法律意见作出详细交代。公民党议员郭荣铿批评，郑不向公众交代的做法是「冇承担，冇责任」的表现，不能只把责任交托予公务员。\"\n",
    "a = tokenize_sentence(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['姚', '松', '炎', '、', '周', '庭', '势', '被', '「', 'D', 'Q', '」', '?', ' ', '泛', '民', '质疑', '，', '政府', '再', '取消', '参选人', '资格涉', '政治', '筛选', '，', '要求', '律政司', '司长郑', '若骅', '解释', '法律', '理据', '。', ' ', '有', '报道', '指', '，', '据', '全国人大常委会', '就', '《', '基本法', '》', '第一', '百', '零', '四条', '进行', '的', '释', '法', '，', '代表', '泛民', '参选立', '法会', '港岛及', '九龙西', '补选的香', '港众', '志周', '庭和被', '「', 'D', 'Q', '」', '前', '议员', '姚', '松', '炎', '，', '势', '被', '取消', '参选', '资格', '。', '律政司', '表示', '，', '法律', '政策', '专员黄', '惠冲', '将于', '稍后时', '间与', '泛民', '议员会面', '，', '确实', '时间', '待定', '。', ' ', '民主派', '议员前', '晚在', '律政', '中心外', '静坐要', '求与', '律政', '司司', '长郑', '若骅', '会面不', '果后', '，', '昨在', '立法会', '召开', '记者招待会', '，', '要求', '郑', '就', '撤销', '参选人', '资格', '的', '理据', '，', '及其', '给予', '选举', '主任', '的', '法律', '意见', '作出详', '细交代', '。', '公民', '党', '议员', '郭', '荣铿', '批评', '，', '郑', '不', '向', '公众', '交代', '的', '做法', '是', '「', '冇', '承担', '，', '冇', '责任', '」', '的', '表现', '，', '不能', '只', '把', '责任', '交', '托予', '公务员', '。']\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
