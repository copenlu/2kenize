{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from functools import reduce\n",
    "import mafan\n",
    "from mafan import text\n",
    "import itertools\n",
    "import sys\n",
    "import os\n",
    "bos = \" <bos> \"\n",
    "eos = \" <eos> \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Functions\n",
    "\n",
    "## Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zng(paragraph):\n",
    "    for sent in re.findall(u'[^!?。\\.\\!\\?]+[!?。\\.\\!\\?]?', paragraph, flags=re.U):\n",
    "        yield sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Chinese Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for simplified to traditional mapping dictionary.\n",
    "\n",
    "We have a large dictionary *conversions.txt* that includes words, characters, common phrases, locations and idioms. Each entry contains the traditional chinese word and simplified chinese word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"conversions.txt\", \"r+\", encoding=\"utf-8\")\n",
    "\n",
    "s2t_dict = dict()\n",
    "\n",
    "for line in infile:\n",
    "    line = line.rstrip()\n",
    "    arr = line.split()\n",
    "    trad = arr[0]\n",
    "    sim = arr[1]\n",
    "    if sim not in s2t_dict:\n",
    "        s2t_dict[sim] = [trad]\n",
    "    else:\n",
    "        s2t_dict[sim].append(trad)\n",
    "s2t_dict['-'] = ['-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokeniser is used for identifying dictionary words and phrases in the input sentence. We always prefer longer phrases because it gives more meaning and less translation mappings. Hence we use Byte Pair Encoding (BPE) for identifying words, while BPE candidates are constrained by the defined list of vocabs in the dictionary. Since the longest phrase in the dictionary has 8 characters we start with 8-character phrases and do it backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(sentence, n = 8):\n",
    "    '''\n",
    "    This function tokenizes input sentences according to the dicitionary.\n",
    "    Input: a sentence or paragraph\n",
    "    Output: a list of tokens from the input in order according to the original paragraph; a list of non-chinese characters from the original text.\n",
    "    '''\n",
    "    text, charList = prepare(sentence)\n",
    "    token_list = []\n",
    "    input_text = text\n",
    "    for k in range(n, 0, -1):\n",
    "        candidates = [input_text[i:i + k] for i in range(len(input_text) - k + 1)]\n",
    "        for candidate in candidates:\n",
    "            if candidate in s2t_dict:\n",
    "                token_list.append(candidate)\n",
    "                input_text = re.sub(candidate, '', input_text)\n",
    "    final = sequencer(token_list, text)\n",
    "    return final, charList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_list(sentence_list, char_list):\n",
    "    count = 0\n",
    "    original = [] # sentence we want to output\n",
    "    \n",
    "    for word in sentence_list:\n",
    "        if \"-\" in word:\n",
    "            original.append(list(char_list[count]))\n",
    "            count += 1\n",
    "        else:\n",
    "            original.append(word)\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(sentence, char_list):\n",
    "    count = 0\n",
    "    original = \"\" # sentence we want to output\n",
    "\n",
    "    for char in list(sentence):\n",
    "        if char == \"-\":\n",
    "            original += char_list[count] # append character if non-chinese\n",
    "            count += 1\n",
    "        else:\n",
    "            original += char # append chinese\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(sentence):\n",
    "    new = \"\" # input to your tokenizer\n",
    "    char_list = [] # punct / english to be omitted\n",
    "\n",
    "    for char in list(sentence):\n",
    "        if text.identify(char) is mafan.NEITHER:\n",
    "            new += \"-\" # sub - with non-chinese chars\n",
    "            char_list.append(char)\n",
    "        else:\n",
    "            new += char\n",
    "\n",
    "    return new, char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencer(tokens, example):\n",
    "\n",
    "    flags = [1] * len(example)\n",
    "    sequence = []\n",
    "    for token in tokens:\n",
    "        for match in re.finditer(token, example):\n",
    "            location = (token, match.span()[0], match.span()[1])\n",
    "            valid = reduce(lambda x,y:x*y, flags[location[1]:location[2]])\n",
    "            if valid:\n",
    "                sequence.append(location)\n",
    "                for i in range(location[1], location[2]):\n",
    "                    flags[i] = 0\n",
    "            else:\n",
    "                continue\n",
    "    sequence.sort(key=lambda x: x[1])\n",
    "    result = [x[0] for x in sequence]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to prepare our corpus.\n",
    "1. We will add paddings (sentinels) to our sentences.\n",
    "2. Take one sentence at a time.\n",
    "3. Change non-chinese words to FW to avoid data explosion.\n",
    "4. Slice the n-grams and add them to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stuff(order):\n",
    "    '''\n",
    "    This function divides the corpus into n-grams and stores them in dictionary.\n",
    "    Input: order of n-gram (like 2 for bi-gram)\n",
    "    Output: none\n",
    "    '''\n",
    "    infile = open(\"hk-zh.txt\", \"r+\") # this contains our corpus\n",
    "    start_padding = bos * order # add padding\n",
    "    end_padding = eos * order\n",
    "\n",
    "    for line in tqdm(infile, total=1314726):\n",
    "        line = line.rstrip()\n",
    "        sentences = list(zng(line)) # tokenize sentence by sentence\n",
    "        for sentence in sentences:\n",
    "            candidate = start_padding + sentence + end_padding # form sentence\n",
    "            word_list = candidate.split()\n",
    "            word_list_tokens = []\n",
    "            for word in word_list:\n",
    "                if not(bool(re.match('^[a-zA-Z0-9]+$', word))):\n",
    "                    word_list_tokens.append(word) # add if not chinese\n",
    "                else:\n",
    "                    word_list_tokens.append(\"FW\") # turn non-chinese (except punc) to FW\n",
    "            word_list = word_list_tokens\n",
    "            ordered = [word_list[i:i + order] for i in range(1, len(word_list) - order)] # extract n-grams through slicing\n",
    "            # for each ngram, convert to tuple and add to dictionary\n",
    "            for ngram in ordered:\n",
    "                ngram = tuple(ngram)\n",
    "                if ngram not in corpus:\n",
    "                    corpus[ngram] = 1\n",
    "                else:\n",
    "                    corpus[ngram] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to extract till trigrams.\n",
    "\n",
    "We want to do 3 iterations, for trigram, bi-gram and then unigram. Each iteration takes 2 minutes. This is only time-consuming part of this code. Once you prep the dictionary, you don't need to do this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dict()\n",
    "start_order = 3\n",
    "for i in range(start_order, 0, -1):\n",
    "    add_stuff(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you made the dictionary, dump it into a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('corpus.pkl', 'wb') as handle:\n",
    "    pickle.dump(corpus, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a way to load a pickle so you don't need to process data everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.pkl', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Candidate Lists\n",
    "\n",
    "1. Tokenize the input.\n",
    "2. Check the mappings of each input.\n",
    "3. Add all possible mappings to candidate list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(sentence):\n",
    "    '''\n",
    "    Returns list of possible mappings.\n",
    "    Input: Simplified chinese sentence\n",
    "    Output: List of lists. Each list has a set of possible traditional chinese tokens\n",
    "    '''\n",
    "    tokens, char_list = tokenizer(sentence)\n",
    "    candidate_list = []\n",
    "    for token in tokens:\n",
    "        candidate_list.append(s2t_dict[token])\n",
    "    candidate_list = output_list(candidate_list, char_list)\n",
    "    return(candidate_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum log-likelihood calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the log likelihood of a sentence with different \\\\(\\alpha\\\\) penalties for unigram, bigram and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = 4526000 # total number of tokens in corpus\n",
    "\n",
    "def prob(word_list, alpha_0 = 0.25, alpha_1 = 0.5, alpha_2 = 1.0):\n",
    "    '''\n",
    "    Computes the log likelihood probability.\n",
    "    Input: A sequence of words in form of list\n",
    "    Output: Log probabilties\n",
    "    '''\n",
    "    word_list = tuple(word_list) # change word list to tuple\n",
    "    if word_list in corpus:\n",
    "        # word found in dictionary\n",
    "        numerator = corpus[word_list] # get the frequency of that word list\n",
    "        denominator = num_tokens # let denominator be num tokens\n",
    "        # cutoff the last word and check whether it's in corpus\n",
    "        if len(word_list[:-1]) > 1 and word_list[:-1] in corpus:\n",
    "            denom_list = word_list[:-1]\n",
    "            denominator = corpus[denom_list]\n",
    "        if len(word_list[:-1]) == 1 and word_list[:-1] in corpus:       \n",
    "            return alpha_0 * log(numerator / denominator) # log of prob*alpha\n",
    "        elif len(word_list[:-1]) == 2 and word_list[:-1] in corpus:\n",
    "            return alpha_1 * log(numerator / denominator)\n",
    "        elif len(word_list[:-1]) == 3 and word_list[:-1] in corpus:\n",
    "            return alpha_2 * log(numerator / denominator)\n",
    "        else:\n",
    "            return log(numerator/denominator)\n",
    "    else:\n",
    "        word_list = list(word_list) # convert it back to list\n",
    "        k = len(word_list) - 1 # backoff, reduce n gram length\n",
    "        if k > 0:\n",
    "            # recursive function, divide the sequence into smaller n and find probs\n",
    "            probs = [prob(word_list[i:i + k]) for i in range(len(word_list) - k + 1)]\n",
    "            return sum(probs)\n",
    "        else:\n",
    "            # we found an unseen word\n",
    "            if not(bool(re.match('^[a-zA-Z0-9]+$', word_list[0]))):\n",
    "                return log(1 / num_tokens) # return a small probability\n",
    "            else:\n",
    "                return prob([\"FW\"]) # we encountered a non-chinese word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backoff Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative n-gram language model that estimates the conditional probability of a word given its history in the n-gram. It's calculated by backing off through progressively shorter history models.\n",
    "\n",
    "# Stupid Backoff\n",
    "\n",
    "Stupid Backoff does not generate normalized probabilities. The main difference is that we don’t apply any discounting and instead directly use the relative frequencies (S is used instead of P to emphasize that these are not probabilities but scores).\n",
    "\n",
    "\\\\(S(w^i|w^{i−1}_{i−k+1}) = \n",
    "\\begin{cases}\n",
    "    \\frac{f(w^{i}_{i−k+1})}{f(w^{i-1}_{i−k+1})} & \\text{if } f(w^{i}_{i−k+1})> 0\\\\\n",
    "    \\\\\\alpha S(w^i|w^{i−1}_{i−k+2}),              & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\\\)\n",
    "\n",
    "Where \\\\(\\alpha\\\\) is the backoff factor.\n",
    "\n",
    "Stupid Backoff is inexpensive to calculate in a distributed environment while approaching the quality\n",
    "of Kneser-Ney smoothing for large amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def backoff(sentence, order, alpha_0 = 0.25, alpha_1 = 0.5, alpha_2 = 1.0):\n",
    "    '''\n",
    "    Calcuates log likelihood using backoff language model\n",
    "    Input: Sentence and order of the n-gram\n",
    "    Output: Log prob of that sentence\n",
    "    '''\n",
    "    score = 0\n",
    "    sentences = list(zng(sentence)) # sentence tokenizer\n",
    "    for sentence in sentences:\n",
    "        start_padding = bos * order # beginning padding\n",
    "        end_padding = eos * order # ending padding\n",
    "        candidate = start_padding + sentence + end_padding # add paddings\n",
    "        word_list = candidate.split()\n",
    "        word_list_tokens = []\n",
    "        for word in word_list:\n",
    "            # append only non-chinese words\n",
    "            if not(bool(re.match('^[a-zA-Z0-9]+$', word))):\n",
    "                word_list_tokens.append(word)\n",
    "            else:\n",
    "                word_list_tokens.append(\"FW\")\n",
    "        word_list = word_list_tokens\n",
    "        ordered = [word_list[i:i + order] for i in range(1, len(word_list) - order)] # shingle into n-grams\n",
    "        probs = [prob(x, alpha_0, alpha_1, alpha_2) for x in ordered] # calculate probabilities\n",
    "        score += sum(probs) # final answer\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take simplified sentence as input, generate candidate list for sentence.\n",
    "For words with many to one mappings add the candidate to a temporary sentence, calculate perplexity and choose the option which gives the lowest perplexity.\n",
    "\n",
    "Call function to add back spaces at the end and output the final sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, alpha_0 = 0.25, alpha_1 = 0.5, alpha_2 = 1.0):\n",
    "    '''\n",
    "    Translate a given sentence to traditional\n",
    "    Input: Simplified Sentence\n",
    "    Output: Traditional Sentence\n",
    "    '''\n",
    "    candidates = convert(sentence) # get the candidate lists\n",
    "    final_sent = \"\"\n",
    "    for words in candidates:\n",
    "        if len(words) > 1:\n",
    "            # many to one mappings\n",
    "            score = -50000.0 # start with extreme negative value\n",
    "            likely = \"\"\n",
    "            for candidate in words:\n",
    "                temp = final_sent\n",
    "                temp = temp + \" \"  + candidate # add a candidate to temp sentence\n",
    "                current_score = backoff(temp, 3, alpha_0, alpha_1, alpha_2) # check perplexity\n",
    "                if current_score > score:\n",
    "                    # if performing good, include that\n",
    "                    score = current_score\n",
    "                    likely = candidate\n",
    "            final_sent = final_sent + \" \" + likely\n",
    "        else:\n",
    "            final_sent = final_sent + \" \" + words[0]\n",
    "    final_sent = final_sent.replace(\" \", \"\")\n",
    "    final_sent = add_back_spaces(sentence, final_sent) #call function to add the spaces back and output translation\n",
    "    return final_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add spaces back by enumerating through the original and the appended list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_back_spaces(original, current):\n",
    "    current_list = list(current)\n",
    "    original_list = list(original)\n",
    "    count = 1\n",
    "    for index, char in enumerate(original_list):\n",
    "        if char == \" \":\n",
    "            current_list[index - count] += \" \"\n",
    "            count += 1\n",
    "    current = \"\".join(current_list)\n",
    "    return current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sentence for translate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"早在23岁，伍兹就参与了世界上首个核反应堆Chicago Pile-1的建设，她是导师费米领导的项目团队中最年轻的一员。此外，伍兹在建立和使用实验所需的盖革计数器上起到关键作用。反应堆成功运转并达到自持状态时，她也是唯一在场的女性。曼哈顿计划中，她与费米合作；同时，她曾与第一任丈夫约翰·马歇尔（John Marshall）一同解决了汉福德区钚生产厂氙中毒的问题，并负责监督钚生产反应炉的建造和运行。\"\n",
    "a = translate(sentence)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the accuracy on 100 lines on sample test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_filename = \"simplified100.txt\"\n",
    "tra_filename = \"traditional100.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = []\n",
    "for key in s2t_dict:\n",
    "    if len(s2t_dict[key]) > 1:\n",
    "        for t in s2t_dict[key]:\n",
    "            checklist.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translated characters are matched with the original traditional corpus during evaluation. The mismatch characters include wrongly characters and variant characters. Variant characters are characters that are homophones and synonyms. In some cases, simplified characters can have multiple traditional variant characters mapped to them, which gives the same meaning and context. Thus a mismatch in this case does not necessarily means a incorrect conversion. However this is not a common case and does not affect the evaluation result substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(sim_filename, tra_filename, alpha_0 = 0.25, alpha_1 = 0.5, alpha_2 = 1.0):\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    micro_total = 0\n",
    "    micro_correct = 0\n",
    "    \n",
    "    sim_file = open(sim_filename, \"r+\", encoding=\"utf-8\")\n",
    "    tra_file = open(tra_filename, \"r+\", encoding=\"utf-8\")\n",
    "    tra_lines = tra_file.readlines()\n",
    "    line_count = 0\n",
    "\n",
    "    for line in sim_file:\n",
    "\n",
    "        line = line.rstrip()\n",
    "\n",
    "        line = translate(line, alpha_0 , alpha_1 , alpha_2)\n",
    "        tra_line = tra_lines[line_count].rstrip()\n",
    "\n",
    "        if len(line) == len(tra_line):\n",
    "            char_count = 0\n",
    "            for c in line:\n",
    "                total = total + 1\n",
    "                if c == tra_line[char_count]:\n",
    "                    correct = correct + 1\n",
    "                else:\n",
    "                    # print(c + tra_line[char_count])\n",
    "                    wrong = wrong + 1\n",
    "\n",
    "                if tra_line[char_count] in checklist:\n",
    "                    micro_total += 1\n",
    "                    if c == tra_line[char_count]:\n",
    "                        micro_correct = micro_correct + 1\n",
    "\n",
    "                char_count = char_count + 1\n",
    "\n",
    "        line_count += 1\n",
    "    results = []\n",
    "    results.append(('Total', (total)))\n",
    "    results.append(('Correct' , (correct)))\n",
    "    results.append(('Wrong' , (wrong)))\n",
    "    results.append(('Percentage' , (correct/total*100)))\n",
    "    results.append(('Micro Total' , (micro_total)))\n",
    "    results.append(('Micro Correct' , (micro_correct)))\n",
    "    results.append(('Micro Percentage' , (micro_correct/micro_total*100)))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Accuracy is defined as (no. of correctly converted characters) / (no. of converted characters). We also calculate the Micro-average accuracy to evaluate the performance for one-to-many character conversions only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval(sim_filename, tra_filename, 0.7, 0.5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the evaluation function on the test set with multiple hyperparameter values in order to determine the optimal values for the hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open('alpha_tuning.txt', 'w+')\n",
    "max = [99.5, 0.25, 0.5, 1]\n",
    "for a_0 in range(100, 10, -10):\n",
    "    for a_1 in range(100, 10, -10):\n",
    "        outfile.write(str(eval(sim_filename, tra_filename, alpha_0 = float(a_0)/100.0, alpha_1 = float(a_1)/100.0, alpha_2 = 1.0)) + \"a0: \" + str(a_0/100.0) + \" \" + \"a1: \" + str(a_1/100.0) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
